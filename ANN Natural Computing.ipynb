{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Natural Computing and Applications\n",
    "# Chapter on Artificial Neural Networks\n",
    "\n",
    "# Prof. Leandro Nunes de Castro\n",
    "# Florida Gulf Coast Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6b018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values: [ 0.5 -1.   2. ]\n",
      "Output value: 0.01297871031485709\n"
     ]
    }
   ],
   "source": [
    "# Generic Artificial Neuron with varying activation functions\n",
    "# chosen among Linear, Threshold, and Sigmoid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs, activation_function='linear'):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def linear_activation(self, x):\n",
    "        return x\n",
    "\n",
    "    def threshold_activation(self, x):\n",
    "        threshold = 0\n",
    "        return 1 if x > threshold else 0\n",
    "\n",
    "    def sigmoid_activation(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation_function == 'linear':\n",
    "            return self.linear_activation(x)\n",
    "        elif self.activation_function == 'threshold':\n",
    "            return self.threshold_activation(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid_activation(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activate(weighted_sum)\n",
    "\n",
    "# Example usage:\n",
    "num_inputs = 3  # Change this value as needed\n",
    "neuron = Neuron(num_inputs, activation_function='linear')\n",
    "\n",
    "# Example input values\n",
    "input_values = np.array([0.5, -1, 2])\n",
    "\n",
    "# Forward pass through the neuron\n",
    "output = neuron.forward(input_values)\n",
    "\n",
    "print(f\"Input values: {input_values}\")\n",
    "print(f\"Output value: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiments to understand the Generic Artificial Neuron\n",
    "\n",
    "# 1. Effect of Changing Activation Functions:\n",
    "#    - Set up the neuron with different activation functions ('linear', 'threshold', 'sigmoid').\n",
    "#    - Provide sample input values and observe the differences in output.\n",
    "#    - Discuss and compare how each activation function affects the output.\n",
    "\n",
    "# 2. Influence of Input Weights:\n",
    "#    - Keep the activation function constant (e.g., 'sigmoid').\n",
    "#    - Vary the weights assigned to each input and observe the impact on the output.\n",
    "#    - Discuss how different weights affect the behavior of the neuron.\n",
    "\n",
    "# 3. Threshold Activation Experiment:\n",
    "#    - Focus on the threshold activation function.\n",
    "#    - Adjust the threshold value and observe changes in the output.\n",
    "#    - Discuss the role of the threshold in determining the neuron's response.\n",
    "\n",
    "# 4. Random Initialization Sensitivity:\n",
    "#    - Run the experiment multiple times with the same parameters to observe variations in outputs due to random weight and bias initialization.\n",
    "#    - Discuss the importance of random initialization in training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f603697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing AND gate:\n",
      "AND([0 0]) = 0, Expected: 0\n",
      "AND([0 1]) = 0, Expected: 0\n",
      "AND([1 0]) = 0, Expected: 0\n",
      "AND([1 1]) = 1, Expected: 1\n",
      "\n",
      "Testing OR gate:\n",
      "OR([0 0]) = 0, Expected: 0\n",
      "OR([0 1]) = 1, Expected: 1\n",
      "OR([1 0]) = 1, Expected: 1\n",
      "OR([1 1]) = 1, Expected: 1\n",
      "\n",
      "Testing NOT gate:\n",
      "NOT([0]) = 1, Expected: 1\n",
      "NOT([1]) = 0, Expected: 0\n"
     ]
    }
   ],
   "source": [
    "# Simple Perceptron for Boolean Functions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs, activation_function='threshold'):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def threshold_activation(self, x):\n",
    "        threshold = 0\n",
    "        return 1 if x > threshold else 0\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation_function == 'threshold':\n",
    "            return self.threshold_activation(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activate(weighted_sum)\n",
    "\n",
    "# Function to test a logic gate\n",
    "def test_logic_gate(neuron, gate_name, truth_table):\n",
    "    print(f\"\\nTesting {gate_name} gate:\")\n",
    "    for input_values, expected_output in truth_table:\n",
    "        output = neuron.forward(input_values)\n",
    "        print(f\"{gate_name}({input_values}) = {output}, Expected: {expected_output}\")\n",
    "        assert output == expected_output, f\"Test failed for {gate_name} gate with inputs {input_values}\"\n",
    "\n",
    "# Truth tables for logic gates\n",
    "and_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 0),\n",
    "    (np.array([1, 0]), 0),\n",
    "    (np.array([1, 1]), 1),\n",
    "]\n",
    "\n",
    "or_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 1),\n",
    "    (np.array([1, 0]), 1),\n",
    "    (np.array([1, 1]), 1),\n",
    "]\n",
    "\n",
    "not_truth_table = [\n",
    "    (np.array([0]), 1),\n",
    "    (np.array([1]), 0),\n",
    "]\n",
    "\n",
    "xor_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 1),\n",
    "    (np.array([1, 0]), 1),\n",
    "    (np.array([1, 1]), 0),\n",
    "]\n",
    "\n",
    "# Neurons for logic gates\n",
    "and_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "and_neuron.weights = np.array([0.5, 0.5])\n",
    "and_neuron.bias = -0.7\n",
    "\n",
    "or_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "or_neuron.weights = np.array([0.5, 0.5])\n",
    "or_neuron.bias = -0.2\n",
    "\n",
    "not_neuron = Neuron(num_inputs=1, activation_function='threshold')\n",
    "not_neuron.weights = np.array([-1])\n",
    "not_neuron.bias = 0.5\n",
    "\n",
    "#xor_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "\n",
    "# Testing logic gates\n",
    "test_logic_gate(and_neuron, 'AND', and_truth_table)\n",
    "test_logic_gate(or_neuron, 'OR', or_truth_table)\n",
    "test_logic_gate(not_neuron, 'NOT', not_truth_table)\n",
    "#test_logic_gate(xor_neuron, 'XOR', xor_truth_table)  # Placeholder for XOR, won't be accurate with a single-layer perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiments to understand the Simple Perceptron\n",
    "\n",
    "# 1. Modify Thresholds and Biases:\n",
    "#    - Experiment with different threshold values and biases for each logic gate.\n",
    "#    - Observe how changing these parameters affects the performance of the perceptron.\n",
    "#    - Discuss the impact on decision boundaries.\n",
    "\n",
    "# 2. Implementation of XOR Gate:\n",
    "#    - Uncomment the XOR neuron and set appropriate weights and biases.\n",
    "#    - Test the XOR gate using the provided truth table.\n",
    "#    - Discuss why a single-layer perceptron struggles to learn XOR compared to AND, OR, and NOT gates.\n",
    "\n",
    "# 3. Generalization Testing:\n",
    "#    - Create additional truth tables for different boolean functions.\n",
    "#    - Test the existing perceptrons (AND, OR, NOT) with these new truth tables.\n",
    "#    - Discuss the limitations of a single-layer perceptron in solving more complex boolean functions.\n",
    "\n",
    "# 4. Activation Function Exploration:\n",
    "#    - Modify the code to use the sigmoid activation function.\n",
    "#    - Test the perceptrons with the sigmoid activation function and compare the results with the threshold activation function.\n",
    "#    - Discuss the differences in behavior.\n",
    "\n",
    "# 5. Analysis of Decision Boundaries:\n",
    "#    - Plot the decision boundaries for each logic gate using different input values.\n",
    "#    - Discuss how the decision boundaries change based on weights, biases, and activation functions.\n",
    "\n",
    "# 6. Effect of Input Scaling:\n",
    "#    - Scale the input values to a different range (e.g., [0, 1] or [-1, 1]) and observe the impact on the perceptron's performance.\n",
    "#    - Discuss the importance of input scaling in training neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3801acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing AND gate:\n",
      "AND([0 0]) = 0, Expected: 0\n",
      "AND([0 1]) = 0, Expected: 0\n",
      "AND([1 0]) = 0, Expected: 0\n",
      "AND([1 1]) = 1, Expected: 1\n",
      "\n",
      "Testing OR gate:\n",
      "OR([0 0]) = 0, Expected: 0\n",
      "OR([0 1]) = 1, Expected: 1\n",
      "OR([1 0]) = 1, Expected: 1\n",
      "OR([1 1]) = 1, Expected: 1\n",
      "\n",
      "Testing NOT gate:\n",
      "NOT([0]) = 1, Expected: 1\n",
      "NOT([1]) = 0, Expected: 0\n",
      "\n",
      "Testing XOR gate:\n",
      "XOR is not representable with a single-layer perceptron. Use a multi-layer perceptron.\n",
      "XOR is not representable with a single-layer perceptron. Use a multi-layer perceptron.\n",
      "XOR is not representable with a single-layer perceptron. Use a multi-layer perceptron.\n",
      "XOR is not representable with a single-layer perceptron. Use a multi-layer perceptron.\n"
     ]
    }
   ],
   "source": [
    "# Bonus Code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs, activation_function='threshold'):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.weights = np.random.rand(num_inputs)\n",
    "        self.bias = np.random.rand()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def threshold_activation(self, x):\n",
    "        threshold = 0\n",
    "        return 1 if x > threshold else 0\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation_function == 'threshold':\n",
    "            return self.threshold_activation(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activate(weighted_sum)\n",
    "\n",
    "    def is_xor_neuron(self):\n",
    "        return self.num_inputs == 2 and np.array_equal(self.weights, np.array([1, 1])) and self.bias == -1\n",
    "\n",
    "    def forward_xor(self, inputs):\n",
    "        if self.is_xor_neuron():\n",
    "            print(\"XOR is not representable with a single-layer perceptron. Use a multi-layer perceptron.\")\n",
    "            return None\n",
    "        return self.forward(inputs)\n",
    "\n",
    "# Function to test a logic gate\n",
    "def test_logic_gate(neuron, gate_name, truth_table):\n",
    "    print(f\"\\nTesting {gate_name} gate:\")\n",
    "    for input_values, expected_output in truth_table:\n",
    "        output = neuron.forward_xor(input_values)\n",
    "        if output is not None:\n",
    "            print(f\"{gate_name}({input_values}) = {output}, Expected: {expected_output}\")\n",
    "            assert output == expected_output, f\"Test failed for {gate_name} gate with inputs {input_values}\"\n",
    "\n",
    "# Truth tables for logic gates\n",
    "and_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 0),\n",
    "    (np.array([1, 0]), 0),\n",
    "    (np.array([1, 1]), 1),\n",
    "]\n",
    "\n",
    "or_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 1),\n",
    "    (np.array([1, 0]), 1),\n",
    "    (np.array([1, 1]), 1),\n",
    "]\n",
    "\n",
    "not_truth_table = [\n",
    "    (np.array([0]), 1),\n",
    "    (np.array([1]), 0),\n",
    "]\n",
    "\n",
    "xor_truth_table = [\n",
    "    (np.array([0, 0]), 0),\n",
    "    (np.array([0, 1]), 1),\n",
    "    (np.array([1, 0]), 1),\n",
    "    (np.array([1, 1]), 0),\n",
    "]\n",
    "\n",
    "# Neurons for logic gates\n",
    "and_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "and_neuron.weights = np.array([0.5, 0.5])\n",
    "and_neuron.bias = -0.7\n",
    "\n",
    "or_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "or_neuron.weights = np.array([0.5, 0.5])\n",
    "or_neuron.bias = -0.2\n",
    "\n",
    "not_neuron = Neuron(num_inputs=1, activation_function='threshold')\n",
    "not_neuron.weights = np.array([-1])\n",
    "not_neuron.bias = 0.5\n",
    "\n",
    "xor_neuron = Neuron(num_inputs=2, activation_function='threshold')\n",
    "xor_neuron.weights = np.array([1, 1])\n",
    "xor_neuron.bias = -1\n",
    "\n",
    "# Testing logic gates\n",
    "test_logic_gate(and_neuron, 'AND', and_truth_table)\n",
    "test_logic_gate(or_neuron, 'OR', or_truth_table)\n",
    "test_logic_gate(not_neuron, 'NOT', not_truth_table)\n",
    "test_logic_gate(xor_neuron, 'XOR', xor_truth_table)  # Displays an error message for XOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "335d6917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the network for AND function:\n",
      "Input: [0 0], Predicted Output: False\n",
      "Input: [0 1], Predicted Output: True\n",
      "Input: [1 0], Predicted Output: True\n",
      "Input: [1 1], Predicted Output: True\n"
     ]
    }
   ],
   "source": [
    "# Extended Hebbian Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ExtendedHebbianNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def train(self, input_pattern, output_pattern):\n",
    "        # Apply the Extended Hebbian learning rule\n",
    "        self.weights += np.outer(output_pattern, input_pattern)\n",
    "    \n",
    "    def predict(self, input_pattern):\n",
    "        # Calculate the output using the learned weights\n",
    "        return np.dot(self.weights, input_pattern)\n",
    "\n",
    "def boolean_to_binary(boolean_value):\n",
    "    return 1 if boolean_value else 0\n",
    "\n",
    "def binary_to_boolean(binary_value):\n",
    "    return binary_value > 0.5\n",
    "\n",
    "def main():\n",
    "    # Define the boolean functions for testing\n",
    "    boolean_functions = {\n",
    "        \"AND\": lambda a, b: a and b,\n",
    "        \"OR\": lambda a, b: a or b,\n",
    "        \"XOR\": lambda a, b: (a or b) and not (a and b)\n",
    "    }\n",
    "\n",
    "    # Choose the boolean function for training and testing\n",
    "    selected_function = \"AND\"\n",
    "    boolean_function = boolean_functions[selected_function]\n",
    "\n",
    "    # Generate training data\n",
    "    training_data = [\n",
    "        (np.array([boolean_to_binary(a), boolean_to_binary(b)]), np.array([boolean_to_binary(boolean_function(a, b))]))\n",
    "        for a in [0, 1] for b in [0, 1]\n",
    "    ]\n",
    "\n",
    "    # Create and train the Extended Hebbian Network\n",
    "    input_size = 2\n",
    "    output_size = 1\n",
    "    hebbian_net = ExtendedHebbianNetwork(input_size, output_size)\n",
    "\n",
    "    for input_pattern, output_pattern in training_data:\n",
    "        hebbian_net.train(input_pattern, output_pattern)\n",
    "\n",
    "    # Test the network\n",
    "    test_data = [\n",
    "        (np.array([boolean_to_binary(a), boolean_to_binary(b)]))\n",
    "        for a in [0, 1] for b in [0, 1]\n",
    "    ]\n",
    "\n",
    "    print(f\"Testing the network for {selected_function} function:\")\n",
    "    for input_pattern in test_data:\n",
    "        output = hebbian_net.predict(input_pattern)\n",
    "        predicted_boolean = binary_to_boolean(output[0])\n",
    "        print(f\"Input: {input_pattern}, Predicted Output: {predicted_boolean}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ea8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiments to understand the Extended Hebbian Network\n",
    "\n",
    "# 1. Training on Different Boolean Functions:\n",
    "#    - Change the 'selected_function' to \"OR\" or \"XOR\" and observe how well the network adapts.\n",
    "#    - Discuss the differences in the learned weights for each boolean function.\n",
    "\n",
    "# 2. Impact of Input Patterns:\n",
    "#    - Create new input patterns (e.g., (0, 1), (1, 0), (1, 1), (0, 0)) and test the network.\n",
    "#    - Analyze how the network performs on patterns not seen during training.\n",
    "\n",
    "# 3. Introduce Noise in Training Data:\n",
    "#    - Add random noise to the training data (e.g., flip some output bits) and observe the impact on learning.\n",
    "#    - Discuss the network's robustness to noisy training data.\n",
    "\n",
    "# 4. Evaluate Generalization:\n",
    "#    - Train the network on a subset of the input patterns and test on the remaining patterns.\n",
    "#    - Discuss the network's ability to generalize to unseen patterns.\n",
    "\n",
    "# 5. Vary Network Size:\n",
    "#    - Change the size of the output layer (e.g., increase to 2) and observe how it affects learning and prediction.\n",
    "#    - Discuss the implications of having more output units.\n",
    "\n",
    "# 6. Experiment with Non-Boolean Functions:\n",
    "#    - Define and test the network on a non-boolean function (e.g., multiplication or addition).\n",
    "#    - Discuss the limitations and potential applications of the Extended Hebbian Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21ad3e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter input type (binary or boolean): boolean\n",
      "Enter boolean function (AND, OR, XOR): xor\n",
      "Testing the network for XOR function:\n",
      "Input: [-1 -1], Predicted Output: False\n",
      "Input: [-1  1], Predicted Output: False\n",
      "Input: [ 1 -1], Predicted Output: False\n",
      "Input: [1 1], Predicted Output: False\n"
     ]
    }
   ],
   "source": [
    "# Bonus Code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ExtendedHebbianNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def train(self, input_pattern, output_pattern):\n",
    "        # Apply the Extended Hebbian learning rule\n",
    "        self.weights += np.outer(output_pattern, input_pattern)\n",
    "    \n",
    "    def predict(self, input_pattern):\n",
    "        # Calculate the output using the learned weights\n",
    "        return np.dot(self.weights, input_pattern)\n",
    "\n",
    "def boolean_to_binary(boolean_value):\n",
    "    return 1 if boolean_value else -1\n",
    "\n",
    "def binary_to_boolean(binary_value):\n",
    "    return binary_value > 0\n",
    "\n",
    "def get_user_input(message):\n",
    "    return input(message)\n",
    "\n",
    "def main():\n",
    "    # Define the boolean functions for testing\n",
    "    boolean_functions = {\n",
    "        \"AND\": lambda a, b: a and b,\n",
    "        \"OR\": lambda a, b: a or b,\n",
    "        \"XOR\": lambda a, b: (a or b) and not (a and b)\n",
    "    }\n",
    "\n",
    "    # Get user input for input type (binary or boolean)\n",
    "    input_type = get_user_input(\"Enter input type (binary or boolean): \").lower()\n",
    "\n",
    "    if input_type not in [\"binary\", \"boolean\"]:\n",
    "        print(\"Invalid input type. Please enter 'binary' or 'boolean'.\")\n",
    "        return\n",
    "\n",
    "    # Get user input for the boolean function\n",
    "    selected_function = get_user_input(\"Enter boolean function (AND, OR, XOR): \").upper()\n",
    "\n",
    "    if selected_function not in [\"AND\", \"OR\", \"XOR\"]:\n",
    "        print(\"Invalid boolean function. Please enter 'AND', 'OR', or 'XOR'.\")\n",
    "        return\n",
    "\n",
    "    # Generate training data\n",
    "    if input_type == \"boolean\":\n",
    "        training_data = [\n",
    "            (np.array([boolean_to_binary(a), boolean_to_binary(b)]), boolean_to_binary(boolean_functions[selected_function](a, b)))\n",
    "            for a in [False, True] for b in [False, True]\n",
    "        ]\n",
    "    else:\n",
    "        training_data = [\n",
    "            (np.array([a, b]), boolean_to_binary(boolean_functions[selected_function](binary_to_boolean(a), binary_to_boolean(b))))\n",
    "            for a in [0, 1] for b in [0, 1]\n",
    "        ]\n",
    "\n",
    "    # Create and train the Extended Hebbian Network\n",
    "    input_size = 2\n",
    "    output_size = 1\n",
    "    hebbian_net = ExtendedHebbianNetwork(input_size, output_size)\n",
    "\n",
    "    for input_pattern, output_pattern in training_data:\n",
    "        hebbian_net.train(input_pattern, output_pattern)\n",
    "\n",
    "    # Test the network\n",
    "    if input_type == \"boolean\":\n",
    "        test_data = [\n",
    "            np.array([boolean_to_binary(a), boolean_to_binary(b)])\n",
    "            for a in [False, True] for b in [False, True]\n",
    "        ]\n",
    "    else:\n",
    "        test_data = [\n",
    "            np.array([a, b])\n",
    "            for a in [0, 1] for b in [0, 1]\n",
    "        ]\n",
    "\n",
    "    print(f\"Testing the network for {selected_function} function:\")\n",
    "    for input_pattern in test_data:\n",
    "        output = hebbian_net.predict(input_pattern)\n",
    "        predicted_boolean = binary_to_boolean(output[0])\n",
    "        print(f\"Input: {input_pattern}, Predicted Output: {predicted_boolean}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbd8081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter input type (binary or boolean): boolean\n",
      "Enter boolean function (AND, OR, XOR): xor\n",
      "Enter weight initialization (zeros or random): random\n",
      "Testing the network for XOR function:\n",
      "Input: [-1 -1], Predicted Output: True\n",
      "Input: [-1  1], Predicted Output: False\n",
      "Input: [ 1 -1], Predicted Output: True\n",
      "Input: [1 1], Predicted Output: False\n"
     ]
    }
   ],
   "source": [
    "# Simple Perceptron Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SinglePerceptronNetwork:\n",
    "    def __init__(self, input_size, output_size, initialize_weights='zeros'):\n",
    "        if initialize_weights == 'zeros':\n",
    "            self.weights = np.zeros((output_size, input_size))\n",
    "        elif initialize_weights == 'random':\n",
    "            self.weights = np.random.rand(output_size, input_size) * 0.1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for initialize_weights. Use 'zeros' or 'random'.\")\n",
    "    \n",
    "    def train(self, input_pattern, output_pattern, learning_rate=0.1):\n",
    "        # Apply the single perceptron learning rule\n",
    "        predicted_output = self.predict(input_pattern)\n",
    "        self.weights += learning_rate * np.outer(output_pattern - predicted_output, input_pattern)\n",
    "    \n",
    "    def predict(self, input_pattern):\n",
    "        # Calculate the output using the learned weights\n",
    "        return np.dot(self.weights, input_pattern)\n",
    "\n",
    "def boolean_to_binary(boolean_value):\n",
    "    return 1 if boolean_value else -1\n",
    "\n",
    "def binary_to_boolean(binary_value):\n",
    "    return binary_value > 0\n",
    "\n",
    "def get_user_input(message):\n",
    "    return input(message)\n",
    "\n",
    "def main():\n",
    "    # Define the boolean functions for testing\n",
    "    boolean_functions = {\n",
    "        \"AND\": lambda a, b: a and b,\n",
    "        \"OR\": lambda a, b: a or b,\n",
    "        \"XOR\": lambda a, b: (a or b) and not (a and b)\n",
    "    }\n",
    "\n",
    "    # Get user input for input type (binary or boolean)\n",
    "    input_type = get_user_input(\"Enter input type (binary or boolean): \").lower()\n",
    "\n",
    "    if input_type not in [\"binary\", \"boolean\"]:\n",
    "        print(\"Invalid input type. Please enter 'binary' or 'boolean'.\")\n",
    "        return\n",
    "\n",
    "    # Get user input for the boolean function\n",
    "    selected_function = get_user_input(\"Enter boolean function (AND, OR, XOR): \").upper()\n",
    "\n",
    "    if selected_function not in [\"AND\", \"OR\", \"XOR\"]:\n",
    "        print(\"Invalid boolean function. Please enter 'AND', 'OR', or 'XOR'.\")\n",
    "        return\n",
    "\n",
    "    # Get user input for weight initialization\n",
    "    initialize_weights = get_user_input(\"Enter weight initialization (zeros or random): \").lower()\n",
    "\n",
    "    if initialize_weights not in [\"zeros\", \"random\"]:\n",
    "        print(\"Invalid weight initialization. Please enter 'zeros' or 'random'.\")\n",
    "        return\n",
    "\n",
    "    # Generate training data\n",
    "    if input_type == \"boolean\":\n",
    "        training_data = [\n",
    "            (np.array([boolean_to_binary(a), boolean_to_binary(b)]), boolean_to_binary(boolean_functions[selected_function](a, b)))\n",
    "            for a in [False, True] for b in [False, True]\n",
    "        ]\n",
    "    else:\n",
    "        training_data = [\n",
    "            (np.array([a, b]), boolean_to_binary(boolean_functions[selected_function](binary_to_boolean(a), binary_to_boolean(b))))\n",
    "            for a in [0, 1] for b in [0, 1]\n",
    "        ]\n",
    "\n",
    "    # Create and train the Single Perceptron Network\n",
    "    input_size = 2\n",
    "    output_size = 1\n",
    "    perceptron_net = SinglePerceptronNetwork(input_size, output_size, initialize_weights)\n",
    "\n",
    "    for input_pattern, output_pattern in training_data:\n",
    "        perceptron_net.train(input_pattern, output_pattern)\n",
    "\n",
    "    # Test the network\n",
    "    if input_type == \"boolean\":\n",
    "        test_data = [\n",
    "            np.array([boolean_to_binary(a), boolean_to_binary(b)])\n",
    "            for a in [False, True] for b in [False, True]\n",
    "        ]\n",
    "    else:\n",
    "        test_data = [\n",
    "            np.array([a, b])\n",
    "            for a in [0, 1] for b in [0, 1]\n",
    "        ]\n",
    "\n",
    "    print(f\"Testing the network for {selected_function} function:\")\n",
    "    for input_pattern in test_data:\n",
    "        output = perceptron_net.predict(input_pattern)\n",
    "        predicted_boolean = binary_to_boolean(output[0])\n",
    "        print(f\"Input: {input_pattern}, Predicted Output: {predicted_boolean}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a99b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments to understand the Simple Perceptron Network\n",
    "\n",
    "# 1. Effect of Learning Rate:\n",
    "#    - Modify the learning rate in the 'train' method and observe its impact on the convergence speed and final weights.\n",
    "#    - Discuss the trade-offs between larger and smaller learning rates.\n",
    "\n",
    "# 2. Testing on Other Boolean Functions:\n",
    "#    - Change the boolean function to \"OR\" or \"XOR\" and observe how well the perceptron adapts.\n",
    "#    - Discuss the challenges the perceptron might face in learning more complex functions.\n",
    "\n",
    "# 3. Visualizing Decision Boundary:\n",
    "#    - For 2D inputs, visualize the decision boundary created by the learned weights.\n",
    "#    - Discuss how the decision boundary changes during training.\n",
    "\n",
    "# 4. Initialization Impact:\n",
    "#    - Compare the performance of the perceptron with zero-initialized weights and randomly initialized weights.\n",
    "#    - Discuss how initialization impacts convergence and final weights.\n",
    "\n",
    "# 5. Testing with Different Input Types:\n",
    "#    - Change the input type (binary or boolean) and observe the network's behavior.\n",
    "#    - Discuss any differences in convergence or final accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48303560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Pattern: 0, Predicted Pattern: 0\n",
      "Actual Pattern: 1, Predicted Pattern: 1\n",
      "Actual Pattern: 2, Predicted Pattern: 2\n",
      "Actual Pattern: 3, Predicted Pattern: 3\n",
      "Actual Pattern: 4, Predicted Pattern: 4\n",
      "Actual Pattern: 5, Predicted Pattern: 5\n",
      "Actual Pattern: 6, Predicted Pattern: 6\n",
      "Actual Pattern: 7, Predicted Pattern: 7\n",
      "Actual Pattern: 8, Predicted Pattern: 8\n",
      "Actual Pattern: 9, Predicted Pattern: 9\n"
     ]
    }
   ],
   "source": [
    "# Single Layer Perceptron (SLP) network for pattern recognition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PerceptronNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(output_size, input_size)\n",
    "    \n",
    "    def train(self, input_pattern, target_output, learning_rate=0.1):\n",
    "        predicted_output = self.predict(input_pattern)\n",
    "        self.weights += learning_rate * np.outer(target_output - predicted_output, input_pattern)\n",
    "    \n",
    "    def predict(self, input_pattern):\n",
    "        return np.dot(self.weights, input_pattern)\n",
    "\n",
    "def add_noise(input_pattern, noise_level): # Binary inputs\n",
    "    noisy_pattern = np.copy(input_pattern)\n",
    "    num_bits_to_flip = int(noise_level * len(input_pattern))\n",
    "    indices_to_flip = np.random.choice(len(input_pattern), num_bits_to_flip, replace=False)\n",
    "    noisy_pattern[indices_to_flip] = 1 - noisy_pattern[indices_to_flip]  # Flip bits\n",
    "    return noisy_pattern\n",
    "\n",
    "def generate_training_data():\n",
    "    training_data = []\n",
    "    for i in range(10):\n",
    "        input_pattern = np.zeros(120)\n",
    "        input_pattern[i*12:(i+1)*12] = 1  \n",
    "        target_output = np.zeros(10)\n",
    "        target_output[i] = 1\n",
    "        training_data.append((input_pattern, target_output))\n",
    "    return training_data\n",
    "\n",
    "# Generate training data\n",
    "training_data = generate_training_data()\n",
    "\n",
    "# Create and train the Perceptron Network\n",
    "input_size = 120  # 12x10 resolution\n",
    "output_size = 10  # 10 output neurons, one for each input (0 to 9)\n",
    "perceptron_net = PerceptronNetwork(input_size, output_size)\n",
    "\n",
    "for input_pattern, target_output in training_data:\n",
    "    perceptron_net.train(input_pattern, target_output)\n",
    "    #print(input_pattern) # Print the input pattern\n",
    "    #print(target_output) # Print the target output\n",
    "    \n",
    "# Test the network with noise\n",
    "noise_level = 0  # You can adjust the noise level as needed\n",
    "for input_pattern, target_output in training_data:\n",
    "    noisy_input_pattern = add_noise(input_pattern, noise_level)\n",
    "    output = perceptron_net.predict(noisy_input_pattern)\n",
    "    predicted_digit = np.argmax(output)\n",
    "    actual_digit = np.argmax(target_output)\n",
    "    print(f\"Actual Pattern: {actual_digit}, Predicted Pattern: {predicted_digit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe67d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample experiments to understand the Simple Perceptron Network for Pattern Recognition\n",
    "\n",
    "# 1. Reading Input Patterns:\n",
    "#     - Modify the Single Layer Perceptron algorithm above to read the input file named char_8_12x10.txt and learn to recognize them. (See bonus code below)\n",
    "#     - The network must have one output neuron for each input pattern.\n",
    "\n",
    "# 2. Adding Noise to Data:\n",
    "#     - Introduce random noise to the test data and observe how well the perceptron generalizes. (See bonus code below)\n",
    "#     - Discuss the robustness of the perceptron to noisy data. Test different noise levels: {0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0907e52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bonus Code to read the input char8_12x10.txt file and plot a picture with the characters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_binary_images(matrix):\n",
    "    num_characters = len(matrix)\n",
    "    num_rows = 2\n",
    "    num_cols = (num_characters + 1) // num_rows\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, bitstring in enumerate(matrix, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        plot_binary_image(bitstring)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_binary_image(bitstring):\n",
    "    rows = 12\n",
    "    cols = 10\n",
    "    bits_matrix = np.array(list(bitstring), dtype=int).reshape(rows, 10)\n",
    "\n",
    "    plt.imshow(bits_matrix, cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Assuming characters are separated by spaces in each line\n",
    "    matrix = [list(line.split()) for line in lines]\n",
    "    \n",
    "    # Transpose the matrix to have characters in columns\n",
    "    transposed_matrix = list(map(list, zip(*matrix)))\n",
    "    \n",
    "    return transposed_matrix\n",
    "\n",
    "# Replace 'your_file_path.txt' with the path to your .txt file\n",
    "file_path = 'char8_12x10.txt'\n",
    "boolean_matrix = read_txt_file(file_path)\n",
    "#print(boolean_matrix)\n",
    "\n",
    "# Plot the binary images\n",
    "plot_binary_images(boolean_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed3cc530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input patterns:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy patterns with noise Level = 0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bonus Code\n",
    "# Code to plot a matrix of characters with noise\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_binary_images(matrix):\n",
    "    num_characters = len(matrix)\n",
    "    num_rows = 2\n",
    "    num_cols = (num_characters + 1) // num_rows\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, bitstring in enumerate(matrix, 1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        plot_binary_image(bitstring)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_binary_image(bitstring):\n",
    "    rows = 5\n",
    "    cols = len(bitstring) // 3\n",
    "    bits_matrix = np.array(list(bitstring), dtype=int).reshape(rows, 3)\n",
    "\n",
    "    plt.imshow(bits_matrix, cmap='gray', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "\n",
    "def insert_noise(bitstring, noise_level):\n",
    "#     \"\"\"\n",
    "#     Insert random noise into the given bitstring with a specified noise level.\n",
    "#     The noise level is the probability of flipping each bit.\n",
    "#     \"\"\"\n",
    "    noisy_bitstring = \"\"\n",
    "    for bit in bitstring:\n",
    "        if np.random.rand() < noise_level:\n",
    "            noisy_bitstring += '1' if bit == '0' else '0'\n",
    "        else:\n",
    "            noisy_bitstring += bit\n",
    "    return noisy_bitstring\n",
    "\n",
    "# Example usage with the target matrix and noise level\n",
    "target_matrix = [\n",
    "    \"101101101101101\",\n",
    "    \"000110000011000\",\n",
    "    \"000110100110000\",\n",
    "    \"010010000110110\",\n",
    "    \"000011000110000\",\n",
    "    \"000011000010000\",\n",
    "    \"000110110110110\",\n",
    "    \"000010000010000\",\n",
    "    \"000010000110000\",\n",
    "    \"000010010010000\"\n",
    "]\n",
    "\n",
    "noise_level = 0.1  # You can adjust the noise level as needed\n",
    "\n",
    "noisy_matrix = [insert_noise(bitstring, noise_level) for bitstring in target_matrix]\n",
    "\n",
    "print('Original input patterns:')\n",
    "plot_binary_images(target_matrix)\n",
    "print(f\"Noisy patterns with noise Level = {noise_level}\")\n",
    "plot_binary_images(noisy_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef2493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4868ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700125a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874a5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
